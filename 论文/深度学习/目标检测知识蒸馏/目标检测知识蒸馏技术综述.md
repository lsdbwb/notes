# 现有的目标检测知识蒸馏流水线
![[Pasted image 20220627113401.png]]
1. Logit Mimicking ：*分类信息*的知识蒸馏
2. Feature Imitation : 基于不同的知识蒸馏的区域，学生模仿老师中间层的特征(即计算teacher和student特征图之间的L2 loss)
3. Pseudo BBox Regression：学生把老师预测出的bounding box作为*额外的*回归目标

# Logit Mimicking
Knowledge Distillation（KD）最早是针对图像分类而设计的。核心思想是：**用一个预训练的大模型（teacher）去指导一个小模型（student）的学习**。
![[Pasted image 20220628100925.png]]
网络模型输出n个logits z，z经过一个带温度t的softmax变换S(.,t),得到一个概率分布p = S(z, t)

只需要最小化teacher模型的概率分布pT和student模型的概率分布pS，就可以将大模型的知识迁移到小模型

**温度t的选取对分布的影响：**
温度t越高，分布越平缓，知识越"软"，温度t越低，分布越尖锐，知识越"硬"
![[Pasted image 20220628101336.png]]

## 在目标检测中，只使用Logit Mimicking是低效的
- 在不同的数据集中，类别的数量会变化，较少的类别数量可能给student提供不了多少有用的信息
- Logit Mimicking一直以来都只能在分类头（class head），而无法在定位head上操作，这自然忽视了定位知识传递的有效性

# Feature Imitation
Feature imitation 在*师生的特征图上施加监督*，最常见的做法是先将 student 的特征图尺寸与 teacher 特征图对齐，之后再**选择一些感兴趣的区域**作为蒸馏区域。例如：
1. FitNet（ICLR 2015）[3] 在全图上蒸馏；
2. Fine-Grained（CVPR 2019）[4] 在一些 anchor box 的 location 上蒸馏；
3. 还有 DeFeat（CVPR 2021）[5] 在 GT box 内部用小 loss weight，在 GT box 外部用大 loss weight；
4. 亦或者是 GI imitation（CVPR 2021）[6] 的动态蒸馏区域

但无论选择何种区域，最后都是在*蒸馏区域上计算二者的 L2 loss*.

## Feature Imitation的好处
在multi-task learning学习的框架下，特征图相当于多个head的树根，head相当于树的叶子。特征图显然包含了所有叶子需要的知识。因此进行Feature Imitation自然会*同时传递分类和定位知识*，而分类KD无法传递定位知识

## Feature Imitation的缺点
还是它会在蒸馏区域中的每个 location 上同时传递分类知识与定位知识。

分类知识与定位知识的分布是**不同的**。这一点在以往的工作中有提到，例如 Sibling Head（CVPR 2020）[7]。

两种知识的分布不同，因此导致并不是在同一个location传递分类知识和定位知识都有利。如下图所示，在有的区域进行KD和LD都很好，但在有的区域传递分类知识效果好，有的区域传递定位知识好。换言之需要**分而治之，因地制宜**地传递知识
![[Pasted image 20220628103150.png]]


# Pseudo BBox Regression
伪boundingbox 回归，即student把teacher预测出的bbox也当作回归的目标
