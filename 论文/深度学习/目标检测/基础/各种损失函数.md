# Classificition Loss
# Bounding Box Regeression Loss
按时间顺序
1. Smooth L1 Loss
2. IoU Loss
3. GIoU Loss
4. DIoU Loss
5. CIoU Loss

## Smooth L1 Loss
-   Fast RCNN论文提出该方法
![[Pasted image 20220421104742.png]]
- L1 Loss和L2 Loss的优缺点
L1损失函数在x = 0时没有梯度，在x != 0时梯度为常数，因此在极值点附近容易产生震荡。但是对于离群点的处理比L2好（离群点的损失也不会像L2损失一样变得非常大）；L2损失则是在极值点附近梯度较小，容易稳步收敛。
- Smooth L1 Loss结合二者优点
![[Pasted image 20220421105443.png]]
在e较大时，使用L1 Loss的形式，避免离群点的影响
在e较小时，使用L2 Loss的形式，获得较小的梯度（收敛速度），防止在极值点震荡
![[Pasted image 20220421110513.png]]

实际目标检测任务中的smooth L1 Loss：
![[Pasted image 20220421110331.png]]
边框有四个坐标点，分别独立计算smooth L1 Loss然后求和。

使用Smooth L1 Loss作为目标检测边框损失函数的缺点：
1. Smooth L1 Loss假定边框的各个坐标点是独立的，实际上不是
2. Smooth L1 Loss和评估指标IoU不是对应的，多个检测框可能有相同的Smooth L1 Loss，但是IoU却相差很大

## IoU Loss
- IoU的计算:
```python
def box_iou(box1, box2):
    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py
    """
    Return intersection-over-union (Jaccard index) of boxes.
    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.
    Arguments:
        box1 (Tensor[N, 4])
        box2 (Tensor[M, 4])
    Returns:
        iou (Tensor[N, M]): the NxM matrix containing the pairwise
            IoU values for every element in boxes1 and boxes2
    """

    def box_area(box):
        # box = 4xn
        return (box[2] - box[0]) * (box[3] - box[1])

    area1 = box_area(box1.T)
    area2 = box_area(box2.T)

    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)
    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - \
	    torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)
	# iou = inter / (area1 + area2 - inter)
    return inter / (area1[:, None] + area2 - inter)  
   
```
-  本文由旷视提出，发表于2016 ACM
![[Pasted image 20220421111514.png]]
- IoU Loss就是使用预测框和真实框的IoU作为Loss的度量
![[Pasted image 20220421111302.png]]
实际中常常使用 **1 -  IoU** 作为 IoU Loss
- 缺点：
1. 当预测框和真实框不相交时，IoU为0，不能反映预测框和真实框**距离的远近**，此时损失函数不可导，IoU损失优化两个框不相交的情况
2. 当预测框和真实框大小确定时，只要预测框和真实框相交面积相等，此时IoU的值都是一样的，但是相交的形式无法确定。

## GIoU Loss
- 计算公式
![[Pasted image 20220421113022.png]]
Ac : 两个框的最小闭包区域面积  U：两个框的交集
![[Pasted image 20220421113159.png]]
如上图，Ac为黑色虚线代表的面积（**同时包含**了预测框和真实框的最小框的面积）
- 当两个框重合即IoU为1时，Ac = U, GIoU最大也为1
- 当两个框不重合,IoU为0时，两个框之间的距离越远，则Ac越大，无限远时,GIoU为-1（因此GIoU是很好的距离度量指标）
- 当多个预测框和真实框的IoU相等时，当框对齐方向更好时GIoU的值会更高一些（因此GIoU能够反映两个框是如何相交的）
 ![[Pasted image 20220421113949.png]]

## DIoU Loss
**Distance-IoU Loss**
- DIoU Loss的缺点：
 如下图所示，当目标框完全包裹预测框时，GIoU Loss退化为IoU Loss，以下三种情况的GIoU Loss的值都一样。
![[Pasted image 20220421151700.png]]
- DIoU加入中心点归一化距离解决此问题：
- 损失函数公式
![[Pasted image 20220421152038.png]]
如下图所示绿色为真实框，黑色为预测框。灰色曲线是GIoU Loss中提到的真实框和预测框的最小闭包，c即为最小闭包框的对角线的长度。d即为两个框中心点的距离。
![[Pasted image 20220421152053.png]]
当预测框和真实框完全重合时，d = 0,DIoU Loss的值为0
当预测框和真实框完全不重合时，相隔越远,d和c的大小越接近，![[Pasted image 20220421152502.png]]越接近1。DIoU Loss的值趋近于2
当真实框完全包裹预测框时，预测框越靠近真实框**正中心**，d越小，DIoU Loss的值越小。
- 优势：
	1. DIoU Loss可以直接优化2个框直接的距离，比GIoU Loss收敛速度更快
	2. 对于目标框包裹预测框的这种情况，DIoU Loss可以收敛的很快，而GIoU Loss此时退化为IoU Loss收敛速度较慢

## CIoU Loss
**Complete-IoU Loss**
DIoU Loss只考虑了**重叠面积**和**中心点距离**
- CIoU Loss同时考虑了**长宽比**
![[Pasted image 20220421153420.png]]