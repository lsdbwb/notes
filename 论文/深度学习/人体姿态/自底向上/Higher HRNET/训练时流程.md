### 数据集预处理
#### 生成heatmap
- 模型对每张图片的每个人体关键点都需要生成heatmap，并且会生成多分辨率的热图去和模型输出的热图数量对应
	- 比如模型一次性输出了1/4分辨率和1/2分辨率的热图，则数据集也要对应生成这两种分辨率的ground truth热图

#### 生成joints
对于一张图片，生成的joints的shape为(MAX_PEOPLE_NUM， joints_num,2)
- 每个(x,y)中，x表示ground truth的关键点在heatmap(heatmap被展平成一维后)上的位置，计算方式为：
![[Pasted image 20220415155851.png]]
	idx表示一个人的第几个关键点,output_res则是heatmap的大小。
- 该位置的ground truth关键点置信度较高时,y为1

#### 生成mask


### 损失函数
#### detection loss
模型最终的输出：[[测试时后处理流程#^417e8c]]
得到两个heatmap，分别求两个heatmap和两个ground truth heatmap的均方误差。[[Associative Embedding#^16c461]]
```python
	#其中mask使得除了高斯核以外的区域都为0
	loss = ((pred - gt)**2) * mask[:, None, :, :].expand_as(pred)
    loss = loss.mean(dim=3).mean(dim=2).mean(dim=1)
```
可以用factor控制heatmap_loss的比重
`heatmaps_loss = heatmaps_loss * self.heatmaps_loss_factor[idx]`

#### group loss[[Associative Embedding#^de8542]]
- 模型输出的是tags heatmap
 ![[Pasted image 20220415112228.png]]
- ground truth是joints
![[Pasted image 20220415112249.png]]

joints中记录的是图片中某个人的关键点坐标在tags heatmap中的位置loc
每次从tags heatmap中找到loc处的值即为模型预测的该关键点的tag值。

- group loss = pull loss  + push loss
计算pull loss的代码：
`pull = pull + torch.mean((tmp - tags[-1].expand_as(tmp))**2)`
(其中tmp数组中存放的是某个人**多个关键点**的tag预测值,tags[-1]记录的是tmp数组的平均值，即该人的reference embedding)

计算push loss的代码：
```python
size = (num_tags, num_tags)
A = tags.expand(*size)  # tags存放的不同人的reference embedding
B = A.permute(1, 0)

diff = A - B

if self.loss_type == 'exp':
    diff = torch.pow(diff, 2)
    push = torch.exp(-diff)
    push = torch.sum(push) - num_tags
elif self.loss_type == 'max':
    diff = 1 - torch.abs(diff)
    push = torch.clamp(diff, min=0).sum() - num_tags
else:
    raise ValueError('Unkown ae loss type')

	push_loss = push/((num_tags - 1) * num_tags) * 0.5
```
